---
title: "Modèle 'word2vec' pour la construction de vecteurs de mots"
subtitle: 'Projet final de master 1 linguistique-informatique Université de Paris-cité'
pdf-engine: lualatex
latex-auto-install: true
linkcolor: blue
author: 
  - name: Eléonora Khachaturova
  - name: Armand Garrigou
  - name: Léo Rongieras
bibliography: references.bib
mainfont: Linux Libertine O
format:
  titlepage-pdf:
    documentclass: scrbook
    number-sections: true
    toc: true
    lof: true
    lot: true
    classoption: ["oneside", "open=any"]
    titlepage: classic-lined
    titlepage-logo: "img/logo.jpeg"
    titlepage-theme:
      elements: ["\\titleblock", "\\authorblock", "\\vfill", "\\logoblock", "\\footerblock"]
      page-align: "center"
      title-style: "doublelinewide"
      title-fontsize: 30
      title-fontstyle: "uppercase"
      title-space-after: "0.1\\textheight"
      subtitle-fontstyle: ["Large", "textit"]
      author-style: "plain"
      author-sep: "\\hskip1em"
      author-fontstyle: "Large"
      author-space-after: "2\\baselineskip"
      affiliation-style: "numbered-list-with-correspondence"
      affiliation-fontstyle: "large"
      affiliation-space-after: "0pt"
      footer-style: "plain"
      footer-fontstyle: ["large", "textsc"]
      footer-space-after: "0pt"
      logo-size: "0.25\\textheight"
      logo-space-after: "1cm"
---

# Introduction

Le modèle Word2Vec a marqué une étape significative dans le domaine du traitement du langage naturel, en révolutionnant la manière d'obtenir des représentation du sens des mots. Depuis son introduction par @mikolov2013efficient, Word2Vec est devenu l'un des modèles les plus influents et largement adoptés pour la représentation des mots dans les tâches de traitement automatique du langage.

L'objectif de ce rapport est de présenter en détail notre implémentation du modèle Word2Vec et de discuter des résultats obtenus lors de nos expérimentations. Nous aborderons en premier lieu les fondements théoriques du modèle.


Le modèle Word2Vec se base sur l'hypothèse distributionnelle, selon laquelle les mots ayant des contextes similaires ont tendance à partager des significations similaires. En exploitant de vastes corpus de textes, Word2Vec apprend des représentations vectorielles
 denses pour chaque mot, capturant ainsi les relations sémantiques et syntaxiques entre les mots. Ces représentations vectorielles, souvent appelées embeddings, ont été utilisées avec succès dans de nombreuses applications telles que la traduction automatique,
 la recherche d'informations et la classification de documents.


Dans notre projet, nous avons souhaité développer notre propre implémentation du modèle Word2Vec, afin de mieux comprendre son fonctionnement interne. Cette approche "from scratch" nous a permis d'explorer en profondeur les mécanismes de Word2Vec, depuis la
 création des fenêtres contextuelles jusqu'à l'entraînement du réseau neuronal.

Nous avons défini les objectifs suivants pour notre projet : (1) implémenter l'algorithme Word2Vec en utilisant le modèle CBOW, (2) entraîner notre modèle sur un corpus de texte de grande envergure, et (3) évaluer la qualité des embeddings appris, en utilisant différentes mesures de similarité sémantique et en effectuant des tâches d'analogie de mots.

# Approche théorique

Le modèle Word2Vec propose deux architectures principales : Skip-gram et CBOW (Continuous Bag-of-Words). Dans notre implémentation, nous avons choisi de nous concentrer sur l'architecture CBOW. Contrairement au modèle Skip-gram qui prédit les mots environnants à partir d'un mot central, CBOW utilise le contexte environnant pour prédire le mot central. Cette approche nous permet d'apprendre des embeddings de mots en exploitant les relations contextuelles.

Par exemple en supposant une taille de fenetre de deux : pour la phrase "le chat dort les souris chantent", si notre mot central est "dort" notre contexte sera défini par [le, chat, les, souris]. Comme le montre le schéma suivant :

```{mermaid}
%%| fig-cap: "Schéma de la décomposition mot-cible/contexte"
%%| fig-label: fig-cbow
flowchart TB

  id1[le chat dort les souris chantent] -- w --> id[dort]

  id1[le chat dort les souris chantent] --w+1--> id4[les]
  id1[le chat dort les souris chantent] --w+2--> id5[souris]

  subgraph B[" "]
  id4[les]
  id5[souris]
  end

  subgraph Z[" "]
  direction LR
  id[dort]
  end
  id1[le chat dort les souris chantent] -- w-2 --> id2[le]
  id1[le chat dort les souris chantent] -- w-1 --> id3[chat]
  subgraph A[" "]
  id2[le]
  id3[chat]
  end
```

L'idée qu'un mot est déterminé par son contexte est parfaitement explicité dans les modèles de langue comme les modèles $n$-grams, on l'on assume que la probabilité de la présence d'un mot dans une phrase est déterminé par les mots qui précèdent, soit pour un modèle $bi$-gram :  
$$P(w_1, w_2,...,w_n) = \prod_{i=2}^{n} P(w_i | w_{i-1})$$. 

L'intérêt de l'architecture CBOW est qu'elle permet d'exploiter aussi les mots qui apparaissent *après* notre mot cible. Comme le montre notre schéma, contrairement à un modele $n$-grams traditionnel, le contexte *global* est pris en compte comme information. Un des apports majeur du modèle word2vec est donc se passage à la *bi-directionalité*, prendre le contexte de gauche et de droite.

Nous allons donc voir comment CBOW peut apprendre ces probabilités.

Notre modèle prend en entré un vecteur contexte $x$ et retourne un vecteur mot $y$ qui correspond au mot au centre de notre contexte. On définit deux matrices $\mathcal{A^{(c)}} \in \mathbb{R}^{|V| \times n} \text { et } \mathcal{A^{(w)}} \in \mathbb{R}^{n \times |V|}$ On note :

- $w_i$ le mot en position $i$ du vocabulaire $V$

- $\mathcal{A^{(c)}} \in \mathbb{R}^{|V| \times n}$ la matrice des mots en entré où $a_i$ correspond à la $i$-ème ligne de $\mathcal{A^{(c)}}$ c'est à dire le vecteur qui représente le mot en entré $w_i$ 

- $\mathcal{A^{(w)}} \in \mathbb{R}^{n \times |V|}$ la matrice en sortie où $y_i$ correspond à la $i$-ème colone de $\mathcal{A^{(w)}}$ c'est à dire le vecteur qui représente le mot en sortie $w_i$

- $n$ correspond à la dimension arbitraire des embeddings 

Les étapes du fonctionnement du modèle peuvent être décrites de telle sorte:

1. On crée un vecteur d'entré composé des indices $i$ des mots $\in V$ en contexte avec une fenètre de taille $N$ soit  
$v^c = (x^{c-N},...,x^{c-1}, x^{c+1},...,x^{c+N})$

2. On obtient nos embeddings pour ce contexte. Soit $\mathcal{A^{(c)}}v^c$, on obtient une matrice de taille $N \times n$ ou chaque ligne $i$ correspond à l'embeddings du mot en contexte. Chaque vecteur dense est de dimension $n$

3. On veut récupérer la somme des vecteurs appartenant au contexte $C$, c'est à dire la somme des éléments **par colone** de notre matrices $\mathcal{A^{(c)}}v^c$
soit $$\widehat{x} = \sum_{i=1}^{N\times 2}a_i$$

4. Notre vecteur score $z$ est obtenu par $$z = \mathcal{A^{(w)}} \sum_{i=1}^{N\times 2}a_i = \mathcal{A^{(w)}}\widehat{x}$$

5. On retourne ce score transformé en log probabilité soit $\widehat{y} =$ log_softmax$(z)$

On peut donc remarquer que après l'application du $softmax$ on obtient un vecteur $\widehat{y}$ de la taille du vocabulaire $V$, où chaque $\widehat{y_i}$ correspond à la *log-probabilité* que $\widehat{y_i}$ soit le mot cible du contexte en entrée.

On peut le vérifier algébriquement : en effet notre score est obtenu par le scalaire entre la matrice $\mathcal{A^{(w)}} \in \mathbb{R}^{n \times |V|}$ et le vecteur $\widehat{x}$ de taille $n$ on a donc :
$$
\begin{bmatrix}
y_{1,1} & y_{2,1} & ... & y_{|V|, 1} \\
y_{1,2} & ... & ... & ... \\
y_{1,3} & ... & ... & ... \\
... & ... & ... & ... \\ 
... & ... &  ... & ... \\
... & ... & ... & ... \\
y_{1,n} & ... & ... & y_{|V|, n} 
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
. \\
. \\
. \\
x_n
\end{bmatrix} = \begin{bmatrix}
\widehat{y_1} &
\widehat{y_2} &
\widehat{y_3} &
. &
. &
. &
\widehat{y_{|V|}}
\end{bmatrix}
$$

On obtient donc un vecteur $\widehat{y}$ de taille $|V|$

On peut résumer l'ensemble avec le schéma suivant^[On peut noter qu'il n'y pas dans l'architecture CBOW de fonction d'activation, la seule couche cachée du réseau correspond justement à la somme des embeddings des mots en contexte que nous avons décrit, le nombre de neurone correspond simplement au nombre de mot dans le vocabulaire.] : 

![Architecture de modèle CBOW](./img/cbow.png)

en dernier lieu, le *softmax* prend en entré le vecteur de score $z$ de taille $|V|$ et renvoie un vecteur de même dimension, où chaques composants $i$ est défini comme : 
$$
(\text{softmax}(z))_i = \frac{e^{z_{i}}}{\sum_{j=1}^{|V|}e^{z_{i}}}
$$

# Méthodologie et implémentation

Nous implémentons notre modele avec les caractéristiques suivantes :

- utilisations de batch

- pas de negative sampling 

En choisissant une implémentation en batch, on assure une optimisation de computation. Si l'on regarde la définition du *softmax* on peut se rendre compte que ce calcul est en faite computationnellement lourd, il parcourt l'ensemble du vocabulaire **pour tout mot**. Ce calcul est donc particulierement lourd. En utilisant les batchs, on se permet de passer quelques exemples à chaques itérations, réduisant la lourdeur de ce calcul. De fait, l'utilisation d'une telle technique va nous permettre d'entrainer notre modèle bien plus rapidement, et sur de plus gros corpus de texte. 

Le negative sampling est une technique introduite par Mikolov et son équipe. Bien qu'il s'agisse la aussi d'une technique d'optimisation computationnelle, nous ne l'implémenterons pas dans ce projet.

Notre implémtations repose sur l'utilisation du module [`pytorch`](https://pytorch.org). Ce module propose un ensemble de classe et de fonction permettant d'optimiser nos taches et nos modèles bien plus facilemment. Nous verrons dans les parties suivantes comment nous avons utilisé ce module.

## Données d'entrainement

Nous disposons de trois corpus d'entrainement. Deux sont de langue anglaise et directement issue de texte Wikipédia. Nous avons utilisés deux datasets `Wikitext2` et `Wikitext103`^[Les datasets sont par exemple disponibles sur [Hugging Face](https://huggingface.co/datasets/wikitext)] qui est une collection de plus de 100 millions de tokens.

Pour le corpus en langue française, nous disposons d'une partie du corpus *frcow*. Il s'agit d'extraction de texte issue du web. La particularité de ce corpus et qu'il a été pré-traité avec un travail de lemmatisation^[Nous remercions le professeur Olivier Bonami pour nous avoir transmis une partie de ce corpus comprenant son travail de lemmatisation]. 

L'intéret d'avoir sous la main deux corpus de bonnes tailles est de pouvoir comparer l'apprentissage de notre modèle sur deux langues différentes et de deux types différents, un corpus plain text (`Wikitext103`) et un corpus lemmatiser (`frcow`) pour les besoins d'une étude morphologique.

Comme toutes taches de NLP, nous devons dans un premier temps préparer nos données à être passer dans le modèle, c'est que nous allons décrire dans la partie suivante.

## Prétraitement des données

A partir des données nous avons besoins d'extraire : 

- le vocabulaire : un `set` des mots présents dans notre corpus

- un map des mots par leurs indices dans le corpus : un `dictionnaire word2idx` 

- une liste des mots trié par leurs indices : cette liste `idx2word` permet à partir d'un indice de récupérer le mot correspondant 

- pour chaque mot parcouru sur notre corpus, on doit récupérer son contexte : un `tuple` `([context], target)`

Pour récupérer le vocabulaire, `word2idx` et `idx2word` nous utilisons un module particulier de `pytorch` : [`torchtext`](https://pytorch.org/text/stable/index.html). Nous pourrions récuperer ces informations avec des fonctions écrite "à la main", ce que nous avons fait lors des premiers tests de notre projet, cependant, si l'utilisation et la documentation de torchtext est parfois peu explicite et intuitive, ce module permet d'optimiser grandement notre phase de prétraitement. 

Ce module permet aussi de chargé un dataset, dont notre dataset `Wikitext103`, Ce qui est particulierement pratique car il n'est donc pas nécessaire de télécharger en amont notre dataset, et notre programme est exécutable sur la plupart des machines disposant d'une connexion internet, sans se préoccuper de la présence ou de l'emplacement du dataset.

La fonction `build_vocab_from_iterator` de `torchtext` permet d'obtenir depuis un texte un objet `vocab`. `Torchtext` optimise cette opération est permet de récupérer tout ce dont nous avons besoin (`word2idx`, `idx2word`) à partir des attributs de cette objet `vocab`.

Tout notre prétraitement ce fait dans le fichier `Prepro.py`. On doit retenir que on organisant les taches de cette manière, on se retrouve en fin de phase de prétraitrement avec un objet `preprocess` dont les attributs sont : 

- un objet `Vocab` disposant d'un ensemble d'attribut dont :
  
  - un mapping `word2idx`

  - un mapping inverse `idx2word`

```{python}
#| echo: false
import os
import torch

#############################
# Config all parameters here 
#############################

DEVICE = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
RAW_DATA = 'WikiText2' # toy_corpus ou wikitext2 ou frcow 
DATA_PATH = "./corpus/frcow-lemmatized-100000sent.xml" # only for french corpus
MODEL_ID = RAW_DATA
DISPLAY_LOSS = True 



#general parameters
DISPLAY_N_BATCH = 2000
SAVE_N_EPOCH = 1
BATCH_SIZE = 64
N_SAVE = 1
EPOCH = 20
DATA_TYPE = 'train' #train or test or valid

#preprocess parameters
WINDOW_SIZE = 5
MIN_FREQ = 30

# Model parameters
EMBEDDING_DIM = 250
LEARNING_RATE = 0.025

#eval settings
if RAW_DATA == "WikiText103" or "wikitext2":
    TEST_WORDS = ["pain", "paris", "king", "day", "fight"]
if RAW_DATA == "frcow":
    TEST_WORDS = ["paris", "berlin", "guerre", "il", "triste"]

    

##### Main trainer and utils settings

PREPROCESS_DATA_DIR = os.path.join(MODEL_ID, 'preprocessed')
MODEL_DIR = os.path.join(MODEL_ID, "cbow" + str(EMBEDDING_DIM))

from functools import partial
import torch
import re
from torch.utils.data import DataLoader
from torchtext.data import to_map_style_dataset
from torchtext.vocab import vocab
from torchtext.vocab import build_vocab_from_iterator
from torchtext.datasets import WikiText103, WikiText2
from torchtext.data.utils import get_tokenizer

class Preprocess:

    def __init__(self, dataset_name: str,
                 dataset_type: tuple[str]|str,
                 window_size: int,
                 min_freq_vocab: int,
                 batch_size, 
                 tokenizer_name="basic_english"):

        self.window_size = window_size

        train_data, vocab = self.get_dataloader_vocab(dataset_name, dataset_type, batch_size, min_freq_vocab, tokenizer_name)

        self.train_data = train_data
        self.vocab = vocab

    def get_tokenizer(self, tokenizer_name):

        tokenizer = get_tokenizer(tokenizer_name)

        return tokenizer

    def get_data(self, data_name: str, data_type: tuple[str]|str):
        '''
        get data from torchtext language modelling dataset  
        '''

        if data_name == "WikiText2":
            data = WikiText2(split=data_type)

        elif data_name == "WikiText103":
            data = WikiText103(split=data_type)

        data = to_map_style_dataset(data)

        return data

    def vocab_builder(self, data, tokenizer, min_freq_vocab):
        """build vocab from data"""

        vocab = build_vocab_from_iterator(map(tokenizer, data), specials=["<unk>"], min_freq=min_freq_vocab)
        vocab.set_default_index(vocab["<unk>"])
        
        return vocab

    def get_batch(self, batch, text_pipeline):

        '''batch to be use with pytorch dataloader'''

        
        batch_input, batch_output = [], []
        for text in batch:
            text_tokens_ids = text_pipeline(text)
            
            if len(text_tokens_ids) < self.window_size * 2 + 1:
                continue

            for idx in range(len(text_tokens_ids) - self.window_size * 2):
                token_id_sequence = text_tokens_ids[idx : (idx + self.window_size * 2 + 1)]
                output = token_id_sequence.pop(self.window_size)
                input_ = token_id_sequence
                batch_input.append(input_)
                batch_output.append(output)

        batch_input = torch.tensor(batch_input, dtype=torch.long)
        batch_output = torch.tensor(batch_output, dtype=torch.long)
        return batch_input, batch_output
    
    def get_dataloader_vocab(self, dataset_name, dataset_type, batch_size, min_freq_vocab, tokenizer_name, shuffle=True):

        data = self.get_data(dataset_name, dataset_type)
        tokenizer = self.get_tokenizer(tokenizer_name)

        vocab = self.vocab_builder(data, tokenizer, min_freq_vocab) 

        
        text_pipeline = lambda x: vocab(tokenizer(x))

        collate_function = self.get_batch

        train_data = DataLoader(data, batch_size=batch_size, 
                                shuffle=shuffle, 
                                collate_fn=partial(collate_function, text_pipeline=text_pipeline))
        return train_data, vocab


```
On peut obtenir ce dont on a besoin ainsi : 

```{python}
#| label: lst-preproVocab
#| lst-cap: "récupération de word2idx et idx2word comme attribut"

X = Preprocess(RAW_DATA, DATA_TYPE, WINDOW_SIZE, MIN_FREQ, BATCH_SIZE) # <1>
vocab = X.vocab
word2idx = vocab.get_stoi()
idx2word = vocab.get_itos()

```

1. Les constantes passer en argument sont toutes dans un fichier `config.py` qui est importé.


```{python}

print(word2idx["king"])

```
 
 - Un autre attribut de notre attribut de notre objet `preprocess` est simplement nos `train_data`

```{python}
train_data = X.train_data
```
et voici un exemple pour le premier batch, c'est à dire le premiere ensemble d'exemple que l'on peut trouver dans nos `train_data`:
```{python}
#| echo: false
i = 0  
for idx, (context, target) in enumerate(train_data):
  print(f"batch numéro {idx}")
  input = context
  gold_output = target
  print(f"Un exemple d'input: {input}")
  print(f"\nLe gold_label associé: {gold_output}")
  i += 1 
  if i == 1:
    break

```

On comprend ici l'importante de l'utilisation de batch^[Cette étape est particulierement importante. En réalité toute notre tentative d'optimisation repose sur cette idée de calcul matricielle, il a donc été très important pour nous de vérifié comme on le fait ici que l'on utilise bel et bien un systeme matriciel.]. On a donc, pour nos données d'input, plusieurs contextes, en l'occurrence l'indice des mots en contextes, pour un seul élément dans nos `train_data`. Ce qu'on passe en `input`^[Attention : ce ne sont pas encore des matrices d'embeddings, il s'agit des indices des mots en contexte, ce sont les données telles qu'elles sont prêtes à être envoyées dans le modèle.] de notre modele est donc une *matrice* qui comprends un ensemble de contexte. Le *gold_label* est donc par la même logique  devenu un vecteur dont les éléments sont les gold_label associés à chaques contextes de notre matrice.(Dans notre optimisation : le vecteur input est devenu une matrice de plusieurs vecteurs, le mot représentant le gold_label est devenu un vecteur de plusieurs gold_label)

On a donc notre phase de pré-traitement terminée. Nous allons décrire brievement l'implémentation du modèle CBOW.


## Modèle CBOW

Nous avons contruit notre modèle à partir de la classe de base `Module`^[voir [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) pour la documentation officielle] proposée par `pytorch`. Nous pouvons visualiser l'ensemble de notre classe : 

```{python}
#| echo: false

import torch
import torch.nn as nn 
import torch.nn.functional as F 

```

```{python}

class CBOWModeler(nn.Module):
  
  def __init__(self, vocab_size, embedding_dim): 
    super(CBOWModeler, self).__init__()
    self.embeddings = nn.Embedding(vocab_size, embedding_dim) # <1>
    self.linear1 = nn.Linear(embedding_dim, vocab_size) #<2>

    initrange = 0.5 / embedding_dim
    self.embeddings.weight.data.uniform_(-initrange, initrange)

  def forward(self, input):
    '''
    calcul de la somme des contextes à laquelle 
    on applique la transformation linéaire (tensor : [1, len(vocab)])

    returns: log_softmax appliqué à la transformation 

    '''
    embedding = self.embeddings(input)
    embedding = torch.sum(embedding, dim=1) # <3>

    Z_1 = self.linear1(embedding) # <4>
    out = F.log_softmax(Z_1, dim=1) # <5>

    return out

```
1. On retrouve ici notre matrices $\mathcal{A^{(c)}} \in \mathbb{R}^{|V| \times n}$ ou $n$ est la dimmension des embeddings
2. Ici notre matrice $\mathcal{A^{(w)}} \in \mathbb{R}^{n \times V}$
3. la sommes des contextes
4. notre vecteur score $z$ est obtenu par $z = \mathcal{A^{(w)}} \sum_{i=1}^{N\times 2}a_i = \mathcal{A^{(w)}}\widehat{x}$
5. On retourne ce score transformé en log probabilité soit $\widehat{y} =$ log_softmax$(z)$ 


Les classes et méthodes essentielles au programme sont décrites dans la documentation. Cette documentation en `.html` sera jointe au dossier final.
Comme on peut le voir, notre classe suit plus ou moins explicitement ce qui a été décrit dans notre premìere partie. La propagation consiste donc en la somme des contextes ainsi que l'application linaire. Cette dernière retourne un `log_softmax`, un vecteurs comprenant les log_probabilités pour chaques classes, dans notre cas, pour chaque mot du vocabulaire.

Voyons donc en dernier lieu comment nous avons conçu la phase d'entrainement.

## Phase d'entrainement

Tout de passe dans le fichier `trainer.py`. 

# Resultats

## Calculer la qualité des embeddings

Notre objectif dans cet exercice est d'obtenir des embeddings de bonne qualité, c'est à dire représentatifs du sens des mots qu'ils représentent. Pour déterminer si nos embeddings encodent des informations sémantiques, nous avons plusieurs moyen de procéder.

Nous allons nous baser sur la comparaisons de nos vecteurs entre eux, sur des taches particulières. Nous pouvons dans un premier temps nous contenter de visualiser la distribution de nos embeddings dans l'espace vectoriel. Nos embeddings étant de taille 200 nous devons d'abord passer par un algorithme de réduction de dimensions afin de ne garder que les 2 dimensions les plus importantes de notre espace vectoriel. Nous obtenons donc des embeddings de dimension 2 et nous pouvons les visualiser.

![visualisation des embeddings en 2 dimension](./img/tsne2model6.png){width=80%}


Afin de confirmer ces résultats, nous pouvons mettre de coté la réduction de dimension, qui implique forcement une perte d'information. Nous cherchons à utiliser une métrique fiable et constante pour calculer la proximité entre vecteurs. Traditionellement on utilise la distance euclidienne ou la similarité cosinus. 

La distance euclidienne est une mesure de la distance entre deux points dans un espace vectoriel à plusieurs dimensions. La distance euclidienne entre deux points A et B est calculée en prenant la racine carrée de la somme des carrés des différences entre les coordonnées correspondantes des points. Mathématiquement, la formule de la distance euclidienne est la suivante :

$$ ||AB|| = \sqrt{(A_1 - B_1)^2 + (A_1 - B_2)^2 + ... + (A_n - B_n)^2}$$

Le calcul de similarité cosinus mesure l'angle entre deux vecteurs dans l'espace vectoriel. La similarité cosinus est une mesure de similarité normalisée qui varie entre -1 et 1. Une valeur de similarité cosinus proche de 1 indique une similarité élevée entre les vecteurs, tandis qu'une valeur proche de -1 indique une similarité inverse. Il est basé sur la formule mathématique suivante :

$$ cos(A, B) = \frac{(A • B)}{(||A|| \times ||B||)} $$

Ces deux métriques sont pertinentes pour notre tache, elles présentent toutes deux des avantages et des inconvénients. La distance euclidienne permet de calculer les vecteurs les plus proches (littéralement) d'un embedding en particulier. Toutefois, cette distance n'est pas entièremment bornée: elle peut être égale à 0 si deux vecteurs se superposent, mais n'a pas de limite naturelle supérieure. Se problème est résolu par la similarité cosinus qui est naturellement bornée entre -1 et 1. Le seul problème de cette métrique est que deux vecteurs peuvent avoir une similarité cosinus de 1 et pourtant ne pas avoir une distance euclidienne égale à 0. Traditionellement on préfère utiliser la similarité cosinus.

```{python}
#| echo: false

import torch.nn as nn
import numpy as np 
from numpy.linalg import norm

def nearest_neighbour(X, embeddings, k):
    distance = nn.CosineSimilarity()
    dist = distance(X, embeddings)
    all_idx = np.argsort(-dist)[:k]
    all_cos = dist[all_idx]

    return all_idx, all_cos

def k_n_nn(model, words, word_to_idx, idx_to_word, k):
    model.eval()
    matrix = model.embeddings.weight.data.cpu()

    print(f"process to determine the {k} nearest words of {words}")

    for word in words:
        input = matrix[word_to_idx[word]]

        ranking, _ = nearest_neighbour(input, matrix, k=k+1)

        print(word.ljust(10), ' | ', ', '.join([idx_to_word[i] for i in ranking[1:]]))
    
    return {}

import torch, os, pickle, shutil
import torch.nn as nn 
import torch.nn.functional as F 
import torch.optim as optim
import numpy as np 


test = torch.load("../../en/cbow200/model6.pth", map_location=torch.device('cpu'))

idx_to_word = test["idx_to_word"]
word_to_idx = test["word_to_idx"]

model = CBOWModeler(len(idx_to_word), 200)
model.load_state_dict(test["cbow_state_dict"])
cos_distance = nn.CosineSimilarity(dim=0)
def eucli_distance(a, b):
  return norm(a - b)


embeds = model.embeddings.weight.data.cpu()

def vec( word):
    return embeds[word_to_idx[word]]

# inp = vec("jeune") - vec("vieux") + vec("léger")                                
# print('inp.shape: ', inp.shape)
inp1 = vec("france")
imp2 = vec("paris")

ham = vec("ham")


```

Par exemple : 
```{python}
#| echo: false
#| label: tbl-franceParis
#| tbl-cap: comparaison de similarité entre Paris/France et Paris/ham

from IPython.display import Markdown
from tabulate import tabulate

cos_france_paris = np.dot(inp1, imp2)/(norm(inp1)*norm(imp2))
euc_france_paris = eucli_distance(inp1, imp2)

cos_france_ham = np.dot(inp1, ham)/(norm(inp1)*norm(ham))
euc_france_ham = eucli_distance(inp1, ham)

table = [["france/paris", cos_france_paris, euc_france_paris], ["france/ham", cos_france_ham, euc_france_ham]]
Markdown(tabulate(table, headers=["cosine", "euclidien"]))
```

On peut remarquer que selon nos mesure "France" est plus proche de "Paris" que de "ham". Plus proche littéralement comme l'indique la mesure de distance euclidienne mais aussi plus similaire selon notre mesure de cosinus, qui est plus proche de 1. 

La tache sur laquelle nous allons pouvoir vraiment évaluer la qualité de nos embeddings sont les tautologies. Elles sont utilisées dans l'article original qui introduit le modèle Word2Vec (Mikolov & al., 2013). Nos vecteurs sont représentables dans un espace euclidien et en respectent les règles: 

- La distance entre deux points est toujours positive.

- La distance entre deux points est nulle si et seulement si les points sont identiques.

- La distance entre deux points est symétrique, c'est-à-dire que la distance entre A et B est la même que la distance entre B et A.

- La distance entre deux points obéit à l'inégalité triangulaire. Cela signifie que la distance entre deux points A et C ne peut jamais être plus courte que la somme des distances entre A et B, et entre B et C.

Ainsi on peut appliquer correctement les opérations mathématiques simples comme l'addition et la soustraction entre vecteurs. (Mikolov et al, 2013) montre que le modèle Word2Vec permet d'appliquer l'addition entre vecteurs pour combiner les sens de deux mots, et inversement avec la soustraction. Nous arrivons donc au fameux example de leur article: 
    
  > *ROI - HOMME + FEMME = REINE*

Si nous vérifions cette égalité avec notre modèle:

```{python}
#| echo: false
#| label: tbl-tautologyQueen
#| tbl-cap: mots les plus proches de (King-men+women)

from IPython.display import Markdown
from tabulate import tabulate

table = [["monarch", 0.49], ["kings", 0.41], ["queen", 0.41], 
         ["nobility", 0.40],
         ["prince", 0.40]]
Markdown(tabulate(table, headers=["word", "cosine-sim"]))

```

On peut voir que le mot "queen" apparait en troisième position des mots les plus similaire au vecteur résultant du calcul algébrique.

Nous allons à présent vérifier que notre modèle performe des résultats similaires sur une liste de tautologies créée par le groupe de travail de Mikolov. Nous considerons qu'une tautologie est validée si le vecteur attendu en sortie est dans les 5 embeddings les plus proches du vecteur résultant de la soustraction et de l'addition.



Les résultats sont bons, sachant que la random baseline de cet exercice serait de $5 \times \frac{1}{|V|}$, si nous considerons 5 examples par tautologie. Nous pouvons essayer de comparer les performance de notre modèle selon le nombre d'itérations et aussi par rapport à FastText, qui fournit des embeddings entrainés sur la même architecture Word2Vec que la notre.

![accuracy en fonction du nombre d'epoch sur une liste de tautologie](./img/accuracy_plot.png){width=70%}

# Limitation et piste d'amélioration

Comme vu précedemment, notre implémentation de word2vec atteint déjà de bonnes performances. Il existe toutefois de nombreuses pistes à explorer pour améliorer la qualité de nos embeddings. 

Nous pourrions explorer plusieurs types de pré-traitement de notre corpus avant de lancer la vectorisation. Nous avons par exemple décidé de passer notre corpus en minuscule, pour réduire le bruit lié aux tokens en début de phrase, mais cela se fait au détriment d'une perte d'information sur les noms propres nottament: "Bordeaux" devient "bordeaux" et les sens se melangent. Toutefois nous avons pensé que cette opération nous coute moins qu'elle nous rapporte. D'autres techniques de nettoyage du corpus comme la lemmatisation, la suppression des caractères spéciaux ou des nombres pourraient être prises en compte. Le cout de pré-traitement est particulièrement long maintenant que nous somme capables de faire marcher notre modèle sur de grandes quantités de données.

Nous avons fait de nombreux tests sur les hyperparamètres afin de trouver ceux qui puissent allier bonne qualité d'embeddings et coût de l'entrainement (en termes de temps et de ressources).
Nous avons nottament fait varier plusieurs hyperparamètres comme la taille de fenêtre, la dimensionnalité des vecteurs, le nombre d'itérations et le taux d'apprentissage. Toutefois nous n'avons pas gardé de trace de manière systématique des performance de notre modèle selon ces différents paramêtres. Il faudrait à l'avenir procéder de façon plus stricte, en passant par exemple par une grid search pour trouver la combinaison de paramêtres optimale, ou bien random search qui permettrait de trouver une combinaison suffisamment bonne en moins de temps.

Aussi, un axe d'amélioration serait d'augmenter la quantité de données, et de les diversifier. Notre modèle est déjà entrainé sur XXXXXXX exemples, toutefois nous savons que plus les données sont conséquentes et variées et plus notre modèle pourra représenter fidèlement les mots de son vocabulaire. De manière générale, l'augmentation de la taille du corpus est souvent la première solution avancée. Toutefois, cela a un cout que nous ne jugions plus nécessaires vu l'état d'avancement du projet. Aussi, les données ajoutées au corpus d'entrainement doivent être de qualité: la diversité dans les exemples est primordiale car sinon les embeddings ne captureront pas totallement le sens des mots qu'ils encodent. Par exemple l'embedding de "robe" ne pourra pas encoder le sens de "couleur" d'un vin si il n'existe pas dans notre corpus d'entrainement de données parlant de vin. Cette problématique est présente dans le domaine de la désambiguisation lexicale, que plusieurs de nos camarades ont expérimenté cette année en projet de TAL.

Une autre problématique est la gestion des déséquilibres lexicaux. Les mots fréquents ont mécaniquement une meilleure qualité de représentation puisqu'ils sont présents dans une quantité plus grande et plus de contextes diversifiés. A l'inverse, les mots rares auront une qualité de représentation amoindrie. Afin de contrer cela, nous pourrions ne considérer qu'une quantité bornée d'examples par mot de vocabulaire, afin de permettre une couverture plus grande du corpus d'entrainement et ainsi augmenter le nombre d'exemples rencontrés pour les mots rares. Nous pourrions aussi implémenter le négative sampling, qui permet d'augmenter la quantité d'exemples par mot en associant un mot avec des mots aléatoires du vocabulaire et en soustrayant leurs sens (EXEMPLE)

Une des lacunes de notre implémentation vient aussi de la métrique d'évaluation. Elle se repose principalement sur la résolution de tautologies. Cette méthode est choisie car elle permet de juger de la qualité de plusieurs embeddings en même temps, tout en montrant qu'une addition/soustraction de vecteurs est similaire à une addition/soustraction de sens. 
Toutefois, cette métrique est biaisée, nottament car le corpus de ces tautologies comporte de nombreux cas de mots rares. Nous ne considérons pas les tautologies où l'embedding d'un mot n'est pas encodé par notre modèle, mais il subsite de nombreux exemples de mots mal encodés qui ne valident pas les tautologies. C'est notamment le cas du sous corpus des tautologies sur les monnaies par exemple, ou les mots sont assez fréquents pour être encodés mais pas assez pour avoir une représentation fidèle de leurs sens. Une autre proposition de métrique pourrait être de se baser sur un modèle dont on sait que les embeddings sont de bonne qualité. En effet, nous pourrions proposer une métrique qui calculerait l'embedding le plus proche (distance ou similarité) d'un mot x dans notre modèle et de déterminer si il est aussi présent dans les n embeddings les plus proches d'un modèle tiers. Nous pourrions par exemple nous baser sur les embeddings fournis par fastext, dont l'utilisation est simple et rapide. 


Enfin, nous pourrions initialiser nos vecteurs non pas de manière aléatoire mais en récupérant directement ceux fournis par un modèle tiers. Nous pourrions ensuite fine-tuner les embeddings sur notre corpus d'entrainement. Cette technique nous permettrait de spécialiser nos vecteurs sur un corpus spécifique, ce qui pourrait être utile si nous avons besoin de vecteurs spécialisé sur un domaine en particulier. L'exemple du milieu viticole donné au dessus est une bonne représentation d'un cas ou le fine-tuning donnerait de bon résultats. Pouvoir spécialiser des embeddings génériques tels que "robe", "mère", "cru", "sec", "moelleux".

# References
